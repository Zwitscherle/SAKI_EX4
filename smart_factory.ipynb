{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('saki4': venv)"
  },
  "interpreter": {
   "hash": "ad4ad34c08212da66dc6d04eda780af769d3ee710939179ad84caa50910b2cc9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This is the smart factory exercise\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox, mdptoolbox.example\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4, 3)\n(3, 4, 4)\nPolicyIteration:\n(0, 0, 0, 0)\n(0.4740921452567589, 1.4285714285714286, 2.7443609022556394, 11.338985762278037)\n1\nValueIteration:\n(0, 0, 0, 0)\n(0.4622244, 1.417, 2.728424, 11.321488)\n4\n"
     ]
    }
   ],
   "source": [
    "# old example for mdp\n",
    "# Definition of the three different transition probability matrices\n",
    "tpmLazy = np.array([[0, 0.95, 0.04, 0.01],[0, 1, 0, 0],[0, 0.2, 0.8, 0],[0, 0, 0.8, 0.2]])\n",
    "tpmDiligent = np.array([[0, 0.95, 0.04, 0.01],[0, 0.3, 0.7, 0],[0, 0.2, 0.8, 0], [0, 0, 0.8, 0.2]])\n",
    "tpmEDiligent = np.array([[0, 0.95, 0.04, 0.01],[0, 0.1, 0.2, 0.7], [0, 0, 0.7, 0.3],[0, 0, 0.8, 0.2]])\n",
    "\n",
    "# This is an example how the construction of the reward matrix is not working as input for the mdp\n",
    "rewardLazy = np.array([0, 1, 2, 4])\n",
    "rewardDiligent = np.array([0, -5, 2, 4])\n",
    "rewardEDiligent = np.array([0, -20, -10, 4])\n",
    "\n",
    "# This is an example how to build the reward matrix \n",
    "rewardfull = np.array([[0, 0, 0],[1, -1, -10], [2, 2, -1], [10, 10, 10]])\n",
    "print(rewardfull.shape)\n",
    "\n",
    "# Bring all the tpm together\n",
    "tmpFull = np.array([tpmLazy, tpmDiligent, tpmEDiligent])\n",
    "print(tmpFull.shape)\n",
    "\n",
    "# Definition of the mdp with discount factor, maximal iterations, the tranisition probability matrix and the reward matrix\n",
    "mdpresultPolicy = mdptoolbox.mdp.PolicyIteration(tmpFull,rewardfull,0.3, max_iter=100)\n",
    "mdpresultValue = mdptoolbox.mdp.ValueIteration(tmpFull,rewardfull,0.3, max_iter=100)\n",
    "\n",
    "# Run the MDP\n",
    "mdpresultPolicy.run()\n",
    "mdpresultValue.run()\n",
    "\n",
    "\"\"\"-------- HERE ARE THE SOLUTIONS ----------------\"\"\"\n",
    "\n",
    "print('PolicyIteration:')\n",
    "print(mdpresultPolicy.policy)\n",
    "print(mdpresultPolicy.V)\n",
    "print(mdpresultPolicy.iter)\n",
    "\n",
    "print('ValueIteration:')\n",
    "print(mdpresultValue.policy)\n",
    "print(mdpresultValue.V)\n",
    "print(mdpresultValue.iter)"
   ]
  },
  {
   "source": [
    "Hints: define states (1572864) + distance influences reward\n",
    "goal: minimize distance \n",
    "items: white blue red\n",
    "\n",
    "create a warehouse of size 3x3, with a separate start \n",
    "-> first move of the agent is always to 1x1\n",
    "\n",
    "MDP 4-tuple(State, Actions, Probability Transition matrix P_a(s,s'), Reward matrix R_a(s,s'))\n",
    "\n",
    "possible States: <br>\n",
    "start/end point: <br>\n",
    "agent start, agent end, empty, white, blue, red => 6 states <br>\n",
    "storage fields: <br> \n",
    "empty, white, blue, red with <br>\n",
    "=> 4 states, but what is with the agent? <br>\n",
    "would make the 1572864 states"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 5, 5)\n(5, 2)\nPolicyIteration:\n(0, 0, 0, 0, 0)\n(1.1896360520213731, 0.632120173404577, 0.4404005780152569, 0.4404005780152569, 0.3568908156064119)\n1\n"
     ]
    }
   ],
   "source": [
    "# Transition matrix for 2x2 warehouse + start position:\n",
    "# only one direction\n",
    "#tmpStart = np.array([[0, 1, 0, 0, 0],[1, 0, 0, 0, 0]])\n",
    "#tmpOne = np.array([[0, 0, 1, 0, 0],[0, 0, 0, 1, 0]])\n",
    "#tmpTwo = np.array([[0, 0, 0, 0, 1],[0, 0, 1, 0, 0]])\n",
    "#tmpThree = np.array([[0, 0, 0, 0, 1],[0, 0, 0, 1, 0]])\n",
    "#tmpFour = np.array([[1, 0, 0, 0, 0],[0, 0, 0, 0, 1]])\n",
    "# arrange shape differently\n",
    "tmpA = np.array([[0, 1, 0, 0, 0],[0, 0, 1, 0, 0],[0, 0, 0, 0, 1],[0, 0, 0, 0, 1],[1, 0, 0, 0, 0]])\n",
    "tmpB = np.array([[1, 0, 0, 0, 0],[0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1]])\n",
    "\n",
    "#T = np.array([tpmStart, tpmOne, tpmTwo, tpmThree, tpmFour])\n",
    "T = np.array([tmpA, tmpB])\n",
    "print(T.shape)\n",
    "\n",
    "# Reward Matrix:\n",
    "# make a reward which gets worse for each step => goal minimize distance\n",
    "# measure steps from start to storage place for each state:\n",
    "# 1: 1, 2,3: 2, 4: 3 => for reward take negative values\n",
    "# Action A, B in this case\n",
    "#R = np.array([[-1, -2, -3, -3, 0],[0, -2, 0, 0, 0]])\n",
    "# arrange shape\n",
    "R = np.array([[1, 0],[1/2, 1/2],[1/3, 0], [1/3, 0], [0, 0]])\n",
    "#R = np.zeros((2,5))\n",
    "#R[0] = [-1, -2, -3, -3, 0] # action A\n",
    "#R[1] = [0, -2, 0, 0, 0] # action B\n",
    "print(R.shape)\n",
    "\n",
    "# try model:\n",
    "mdpWarehousePolicy = mdptoolbox.mdp.PolicyIteration(T, R, 0.3, max_iter=100)\n",
    "\n",
    "# Run the MDP\n",
    "mdpWarehousePolicy.run()\n",
    "\n",
    "print('PolicyIteration:')\n",
    "print(mdpWarehousePolicy.policy)\n",
    "print(mdpWarehousePolicy.V)\n",
    "print(mdpWarehousePolicy.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.         1.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.33333333 0.         0.33333333 0.         0.33333333 0.\n  0.         0.         0.         0.        ]\n [0.         0.33333333 0.         0.33333333 0.         0.33333333\n  0.         0.         0.         0.        ]\n [0.         0.         0.5        0.         0.         0.\n  0.5        0.         0.         0.        ]\n [0.         0.33333333 0.         0.         0.         0.33333333\n  0.         0.33333333 0.         0.        ]\n [0.         0.         0.25       0.         0.25       0.\n  0.25       0.         0.25       0.        ]\n [0.         0.         0.         0.33333333 0.         0.33333333\n  0.         0.         0.         0.33333333]\n [0.         0.         0.         0.         0.5        0.\n  0.         0.         0.5        0.        ]\n [0.         0.         0.         0.         0.         0.33333333\n  0.         0.33333333 0.         0.33333333]\n [0.         0.         0.         0.         0.         0.\n  0.5        0.         0.5        0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# new attempt 2x2 warehouse:\n",
    "# State is only the state of the warehouse => 4 fields a 7 states (empty = 0, \n",
    "# white_unstore = 1, red_unstore = 2, blue_unstore = 3, \n",
    "# white_store = 4, red_store = 5, blue_store = 6) (7^4 = 2401 states) \n",
    "\n",
    "\n",
    "# T has size (Action, State, State)\n",
    "# Action is the position in the warehouse that is chosen\n",
    "# 4 actions? (1,1)(1,2)(2,1)(2,2)\n",
    " \n",
    "\n",
    "# R has size (State, Action)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mdp\n",
    "# just test\n",
    "#mdp = mdptoolbox.mdp.MDP()"
   ]
  }
 ]
}