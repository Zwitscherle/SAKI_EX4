{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('saki4': venv)"
  },
  "interpreter": {
   "hash": "ad4ad34c08212da66dc6d04eda780af769d3ee710939179ad84caa50910b2cc9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This is the Smart Factory Exercise."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox, mdptoolbox.example\n",
    "import numpy as np\n",
    "from enum import Enum"
   ]
  },
  {
   "source": [
    "Define all items, states and actions of our model. <br>\n",
    "* We have three different items (WHITE, BLUE, RED)\n",
    "* We have four different possible states of each warehouse field (EMPTY, WHITE, BLUE, RED)\n",
    "* We have six possible actions for our agent (STORE and RESTORE in combination with each item color)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['WHITE', 'BLUE', 'RED']\n['WHITE', 'BLUE', 'RED', 'EMPTY']\n['STORE WHITE', 'STORE BLUE', 'STORE RED', 'RESTORE WHITE', 'RESTORE BLUE', 'RESTORE RED']\n"
     ]
    }
   ],
   "source": [
    "items = ['WHITE', 'BLUE', 'RED']\n",
    "fieldStatus = ['WHITE', 'BLUE', 'RED', 'EMPTY']\n",
    "actions = []\n",
    "for operation in ['STORE', 'RESTORE']:\n",
    "    for item in items:\n",
    "        actions.append(operation + ' ' + item)\n",
    "\n",
    "print(items)\n",
    "print(fieldStatus)\n",
    "print(actions)"
   ]
  },
  {
   "source": [
    "Create all fields of the warehouse of size lenght x heigth (in our case 2 x 2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "def createWarehouseFields(length, height):\n",
    "    warehouseFields = []\n",
    "    for i in range(0, length):\n",
    "        for j in range(0, height):\n",
    "            warehouseFields.append((i,j))\n",
    "    return warehouseFields\n",
    "\n",
    "warehouseFields = createWarehouseFields(2, 2)\n",
    "print(warehouseFields)"
   ]
  },
  {
   "source": [
    "Next create all possible states of our warehouse. \n",
    "In our case we have 4 fields with 4 different states each (EMPTY, WHITE, BLUE, RED), which results in 4^4 states."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO dynamic state generation\n",
    "def getStates(warehouseFields, fieldStatus):\n",
    "    # TODO create state array   \n",
    "    possibleStates = len(warehouseFields)**len(fieldStatus)\n",
    "    states = np.zeros(possibleStates)\n",
    "    pass\n",
    "\n",
    "states = getStates(warehouseFields, fieldStatus)"
   ]
  },
  {
   "source": [
    "Create a reward which fits our problem. <br>\n",
    "The reward is higher if the distance our agent has to cover is lower."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create rewards"
   ]
  },
  {
   "source": [
    "Bring it all together now and create the transition and the reward matrix. <br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n [1. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0.]\n ...\n [1. 0. 2. 0. 0. 0.]\n [0. 1. 2. 0. 0. 0.]\n [0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# TODO create Transition and reward matrix\n",
    "transition = np.zeros((len(actions), len(states), len(states)))\n",
    "\n",
    "rewards = np.zeros((len(states), len(actions)))\n"
   ]
  },
  {
   "source": [
    "Finally create the mdp models and evaluate the different classes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PolicyIteration:\n"
     ]
    }
   ],
   "source": [
    "# try model:\n",
    "#mdpWarehousePolicy = mdptoolbox.mdp.PolicyIteration(transition, rewards, 0.3)\n",
    "\n",
    "# Run the MDP\n",
    "#mdpWarehousePolicy.run()\n",
    "\n",
    "print('PolicyIteration:')\n",
    "#print(mdpWarehousePolicy.policy)\n",
    "#print(mdpWarehousePolicy.V)\n",
    "#print(mdpWarehousePolicy.iter)\n",
    "\n",
    "#mdpWarehouseQ = mdptoolbox.mdp.QLearning(transition, rewards, 0.3)\n",
    "\n",
    "# Run the MDP\n",
    "#mdpWarehouseQ.run()\n",
    "\n",
    "#print('Q learning:')\n",
    "#print(mdpWarehouseQ.policy)\n",
    "#print(mdpWarehouseQ.V)"
   ]
  }
 ]
}