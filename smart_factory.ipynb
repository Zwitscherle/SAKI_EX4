{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('saki4': venv)"
  },
  "interpreter": {
   "hash": "ad4ad34c08212da66dc6d04eda780af769d3ee710939179ad84caa50910b2cc9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This is the Smart Factory Exercise."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox, mdptoolbox.example\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "source": [
    "Define all items, states and actions of our model. <br>\n",
    "* We have three different items (WHITE, BLUE, RED)\n",
    "* We have four different possible states of each warehouse field (EMPTY, WHITE, BLUE, RED)\n",
    "* We have six possible actions for our agent (STORE and RESTORE in combination with each item color)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['WHITE', 'BLUE', 'RED']\n['WHITE', 'BLUE', 'RED', 'EMPTY']\n[('STORE', 'WHITE'), ('STORE', 'BLUE'), ('STORE', 'RED'), ('RESTORE', 'WHITE'), ('RESTORE', 'BLUE'), ('RESTORE', 'RED')]\n"
     ]
    }
   ],
   "source": [
    "items = ['WHITE', 'BLUE', 'RED']\n",
    "fieldStatus = ['WHITE', 'BLUE', 'RED', 'EMPTY']\n",
    "operations = ['STORE', 'RESTORE']\n",
    "actions = []\n",
    "for operation in operations:\n",
    "    for item in items:\n",
    "        actions.append((operation, item))\n",
    "\n",
    "# warehouse size n x n (in our case n = 2)\n",
    "n = 2\n",
    "print(items)\n",
    "print(fieldStatus)\n",
    "print(actions)"
   ]
  },
  {
   "source": [
    "Create all fields of the warehouse of size lenght x heigth (in our case 2 x 2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "def createWarehouseFields(length, height):\n",
    "    warehouseFields = []\n",
    "    for i in range(0, length):\n",
    "        for j in range(0, height):\n",
    "            warehouseFields.append((i,j))\n",
    "    return warehouseFields\n",
    "\n",
    "warehouseFields = createWarehouseFields(n, n)\n",
    "print(warehouseFields)"
   ]
  },
  {
   "source": [
    "Next create all possible states of our warehouse. \n",
    "In our case we have 4 fields with 4 different states each (EMPTY, WHITE, BLUE, RED), which results in 4^4 states."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "def getStates(warehouseFields, fieldStatus):  \n",
    "    return itertools.product(fieldStatus, repeat=len(warehouseFields))\n",
    "\n",
    "iterStates = getStates(warehouseFields, fieldStatus)\n",
    "states = []\n",
    "for state in iterStates:\n",
    "  states.append(list(state))\n",
    "\n",
    "print(len(states))"
   ]
  },
  {
   "source": [
    "Create a reward which fits our problem. <br>\n",
    "The reward is higher if the distance our agent has to cover is lower."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.25180384 0.24336554 0.50483062]\n[1.0, 0.25, 0.25, 0.1111111111111111]\n"
     ]
    }
   ],
   "source": [
    "# read statistics from file to calculate better rewards\n",
    "# i = 0: White, i = 1: Blue, i = 2: Red\n",
    "countItems = np.zeros((3))\n",
    "itemsTotal = 0\n",
    "\n",
    "warehouseorder = open('Exercise4_warehousetraining2x2.txt')\n",
    "for line in warehouseorder:\n",
    "    curAction = line.split('\\t')\n",
    "    # curOperation = curAction[0].upper()\n",
    "    curItem = curAction[1].strip('\\n').upper()\n",
    "    if curItem == 'WHITE':\n",
    "        countItems[0] += 1\n",
    "    if curItem == 'BLUE':\n",
    "        countItems[1] += 1\n",
    "    if curItem == 'RED':\n",
    "        countItems[2] += 1\n",
    "    itemsTotal += 1\n",
    "\n",
    "# i = 0: White, i = 1: Blue, i = 2: Red\n",
    "probsItems = countItems / itemsTotal\n",
    "print(probsItems)\n",
    "\n",
    "# reward function based on distance and item probability:\n",
    "def getRewardBasedOnDistanceAndProbability(fieldIndex, item):\n",
    "    x = warehouseFields[fieldIndex]\n",
    "    distance = x[0] + x[1] + 1\n",
    "    prob = 0\n",
    "    if item == 'WHITE':\n",
    "        prob = probsItems[0]\n",
    "    if item == 'BLUE':\n",
    "        prob = probsItems[1]\n",
    "    if item == 'RED':\n",
    "        prob = probsItems[2]\n",
    "    reward = 1/distance * 1/distance * prob\n",
    "    return reward\n",
    "\n",
    "# simple reward function:\n",
    "# get the distance in field from 0,0 (0,0 has already distance 1)\n",
    "# reward is the reciprocal**2 (to punish far away fields even more)\n",
    "def getRewardBasedOnDistance(x):\n",
    "    distance = x[0] + x[1] + 1\n",
    "    reward = 1/distance * 1/distance\n",
    "    return reward\n",
    "\n",
    "rewardVector = [getRewardBasedOnDistance(x) for x in warehouseFields]\n",
    "print(rewardVector)"
   ]
  },
  {
   "source": [
    "Bring it all together now and create the transition and the reward matrix. <br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(6, 256, 256)\n1536.0\n"
     ]
    }
   ],
   "source": [
    "# create Transition and reward matrix\n",
    "def createTransitionAndRewardMatrix(actions, states):\n",
    "    T = np.zeros((len(actions), len(states), len(states)))\n",
    "    R = np.zeros((len(states), len(actions)))\n",
    "\n",
    "    for i in range(len(actions)):\n",
    "        # current operation and item\n",
    "        operation, item = actions[i]\n",
    "\n",
    "        for j in range(len(states)):\n",
    "            curState = states[j]\n",
    "\n",
    "            # if an operation is not valid i.e. store when warehouse is full \n",
    "            # or restore when warehouse is empty, stay in the current state \n",
    "            if ((operation == 'STORE') and ('EMPTY' not in curState)) \\\n",
    "                or ((operation == 'RESTORE') and (item not in curState)):\n",
    "                T[i, j, j] = 1                \n",
    "            else:\n",
    "                possibleFields = []\n",
    "                for k in range(len(curState)):\n",
    "                    # find empty fields where the agent can store the item\n",
    "                    if operation == 'STORE':\n",
    "                        if curState[k] == 'EMPTY':\n",
    "                            possibleFields.append(k)\n",
    "                    # find fields with the requested item\n",
    "                    else:\n",
    "                        if curState[k] == item:\n",
    "                            possibleFields.append(k)\n",
    "                # possible fields can not be empty because of the check above\n",
    "                transitionProbability = 1 / len(possibleFields)\n",
    "                # set probabiltiy in transition matrix for the possible nextStates\n",
    "                for field in possibleFields:\n",
    "                    nextState = curState.copy()\n",
    "                    if operation == 'STORE':\n",
    "                        nextState[field] = item\n",
    "                    else:\n",
    "                        nextState[field] = 'EMPTY'\n",
    "                    nextIndex = states.index(nextState)\n",
    "                    T[i, j, nextIndex] = transitionProbability\n",
    "                    # set reward in R\n",
    "                    # R[nextIndex][i] = rewardVector[field] # old simple reward\n",
    "                    R[nextIndex][i] = getRewardBasedOnDistanceAndProbability(field, item)         \n",
    "    return T, R\n",
    "\n",
    "T, R = createTransitionAndRewardMatrix(actions, states)\n",
    "print(np.shape(T))\n",
    "test = np.sum(T)\n",
    "print(test)"
   ]
  },
  {
   "source": [
    "Finally create the mdp models and evaluate the different classes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PolicyIteration:\n(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 2, 5, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 1, 5, 5, 5, 2, 5, 5, 5, 1, 5, 5, 5, 2, 5, 5, 5, 2, 5, 5, 5, 2, 2, 2, 2, 2, 5, 5, 5, 0, 5, 5, 5, 1, 2, 2, 2, 2, 5, 5, 5, 5)\n(0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879269, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879269, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879269, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879269, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879269, 0.2797820444879268, 0.2797820444879268, 0.2797820444879268, 0.2797820444879269, 0.2797820444879269, 0.2797820444879269, 0.2797820444879269, 0.2797820444879269, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.2704061527590939, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5609229138640903, 0.5153292432704197, 0.10498620792738439, 0.5609229138640903, 0.5609229138640903, 0.5153292432704197, 0.10498620792738439, 0.5188536953242835, 0.5188536953242835, 0.5205573433127613, 0.18028166847826288, 0.14023072846602258, 0.14023072846602258, 0.13425274822333647, 0.08045092603916133, 0.5609229138640903, 0.5609229138640903, 0.5153292432704197, 0.10498620792738439, 0.5609229138640903, 0.5609229138640903, 0.5150714062478768, 0.10240783770195534, 0.5188536953242835, 0.5188536953242835, 0.520544771628363, 0.18028103989404295, 0.14023072846602258, 0.14023072846602258, 0.13400194311959018, 0.07794287500169854, 0.5188536953242836, 0.5188536953242836, 0.5205573433127613, 0.18028166847826288, 0.5188536953242836, 0.5188536953242836, 0.520544771628363, 0.18028103989404295, 0.522870354380848, 0.522870354380848, 0.5228703543808482, 0.18039731903166725, 0.18039731903166722, 0.18039731903166722, 0.18039731903166722, 0.15693157401700117, 0.14023072846602258, 0.14023072846602258, 0.13425274822333647, 0.08045092603916133, 0.14023072846602258, 0.14023072846602258, 0.13400194311959018, 0.07794287500169854, 0.18039731903166722, 0.18039731903166722, 0.18039731903166722, 0.15693157401700114, 0.14023072846602258, 0.14023072846602258, 0.13244013244013245, 0.06232476820712114)\n1\n"
     ]
    }
   ],
   "source": [
    "# 1. Policy Iteration\n",
    "mdpWarehousePolicy = mdptoolbox.mdp.PolicyIteration(T, R, 0.1, max_iter=100)\n",
    "# Run the MDP\n",
    "mdpWarehousePolicy.run()\n",
    "\n",
    "print('PolicyIteration:')\n",
    "print(mdpWarehousePolicy.policy)\n",
    "print(mdpWarehousePolicy.V)\n",
    "print(mdpWarehousePolicy.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q learning:\n(0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 3, 5, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0)\n(0.2797820444879245, 0.27146264270395987, 0.2791788100799231, 0.1630339176027851, 0.2699927426907361, 0.10032835651258708, 0.20718767312812947, 0.016948130825490314, 0.2797381369501302, 0.0, 0.20021324941130064, 0.027742347378467504, 0.1399314353905209, 0.00896177949180866, 0.030991181058545415, 0.01854932015520199, 0.2722990192520395, 0.13383576162363509, 0.21178850780314573, 0.05298483558011481, 0.10647588611487199, 0.09527404797419303, 0.12158667488338927, 0.0025993991905895503, 0.27943885994939033, 0.11905389204798851, 0.12539684527077555, 0.11052589708494241, 0.04221340679800902, 0.0060419751435915004, 0.057269271382369744, 0.0029653885640823436, 0.27900753598055233, 0.15742934340368264, 0.27931531093862055, 0.025918960893536316, 0.012185861629649351, 0.044740094669219084, 0.0, 0.0, 0.26220208395940847, 0.07307255710963947, 0.1489592851713023, 0.016830673012423553, 0.05014781548909165, 0.006118701766925911, 0.050178711679522044, 0.008858428180259696, 0.13807953404595966, 0.005023493397734907, 0.05425623544358326, 0.06151454591598273, 0.013452517414699675, 0.0054365839415408575, 0.017702028819405788, 0.0, 0.029949409030024736, 0.0, 0.03833840807311599, 0.08902610141024221, 0.033565711114924665, 0.0, 0.00619835813948797, 0.007647589001884662, 0.27040201602713065, 0.039423055937641414, 0.00017467229644388115, 0.021471067562068822, 0.04713312267746417, 0.015636094979914, 0.0, 0.002611633084471506, 0.0789729964792725, 0.0, 0.019665726450594726, 0.0011462693735947505, 0.02033704923839538, 0.0014059258531282075, 0.001649760755021799, 0.009934608474262646, 0.030761076880343127, 0.0, 0.0, 0.0, 0.0, 0.12632221444585257, 0.0, 0.0, 0.009473784677185628, 0.0, 0.0, 0.0, 0.0006558192277134795, 0.0, 0.0, 0.0, 0.02360755142241721, 0.035643065763745194, 0.014660802663763435, 0.001716480974848031, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005266930479783609, 0.0, 0.0, 0.0013547267197444091, 0.0172807494526655, 0.0006877217255585539, 0.0034141772176255287, 0.007239562890963959, 0.000596226372660712, 2.3438350929931814e-06, 0.0, 0.0, 0.00033397351466125133, 0.0, 3.724769478868958e-05, 0.0, 0.005815654437729287, 0.0, 5.854999060125617e-06, 0.0, 0.06490199168856446, 0.04003012721217792, 2.159614504622024e-06, 0.002149114850567708, 0.17065392558998183, 0.04014700060834567, 0.0, 0.003037762882308483, 0.01815078754499157, 0.0008486762808573218, 0.0, 0.0009104654141012567, 0.008157210509423586, 0.0008613081978109727, 0.0, 0.001258264467866764, 0.031228071746464282, 0.020269957993050872, 0.0, 0.0026724558169270345, 0.007745958588882289, 0.0, 0.030500916582095007, 0.0, 0.007998941824872891, 0.34737115430110094, 0.0, 0.0, 0.00042977986734027125, 1.3492063315012249e-05, 0.0, 0.0, 0.5575359648833847, 0.01858328497171651, 0.0, 0.01612092141729877, 0.020297044611062694, 0.0, 0.0, 0.0, 0.09518626840712315, 0.0, 0.0, 0.0, 0.003756393703779417, 0.0, 0.0, 7.244954681397526e-06, 0.006417455312002024, 0.002234719651643835, 0.0006326170556813165, 0.0007283555330138827, 0.0016093908418296804, 0.00011449263872729076, 0.0, 0.0, 0.0015523135313234818, 0.0, 0.0, 0.0, 0.0003352894960847758, 0.0, 0.0, 0.0, 0.050625683227416694, 0.007697409355641505, 0.00364059626002774, 0.013531056869297847, 0.005367806899523532, 0.0, 0.0, 0.014412928707895534, 0.009536800933786619, 0.0, 0.0031609448736925785, 0.0018963197153787357, 0.004180287107282611, 0.0, 0.0011286763064976692, 0.014382217403891881, 0.18436677853300748, 0.0016094002987409332, 0.0027712345738582014, 0.006208514432932776, 0.00036095650877647706, 0.0, 0.00020889480704449194, 0.0, 0.002197952876692268, 0.00015623545187087363, 0.00020679759193898252, 0.0, 0.0007533373934181891, 0.0, 0.0, 2.009045474195857e-05, 0.014337742632714036, 0.0011677625747066635, 0.02536411121347769, 0.003981120317547734, 0.00045816683365344926, 3.9524411716555695e-05, 0.0, 0.0, 0.00241933384839249, 0.0, 0.0005209521595272394, 0.0, 0.00039097431070835655, 0.0, 6.448992216998388e-06, 0.0, 0.004687534329641789, 0.0, 0.0016748878243736868, 0.013900163855149148, 0.0003870255191356504, 0.0, 0.0, 0.0, 0.0006562378007190074, 0.0, 7.240034741962651e-05, 6.840636794642948e-07, 0.0014720269403281903, 0.0, 0.0, 0.0001161585902493357)\n"
     ]
    }
   ],
   "source": [
    "# 2. QLearning\n",
    "mdpWarehouseQ = mdptoolbox.mdp.QLearning(T, R, 0.1)\n",
    "# Run the MDP\n",
    "mdpWarehouseQ.run()\n",
    "\n",
    "print('Q learning:')\n",
    "print(mdpWarehouseQ.policy)\n",
    "print(mdpWarehouseQ.V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Value Iteration:\n(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 2, 5, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 1, 5, 5, 5, 2, 5, 5, 5, 1, 5, 5, 5, 2, 5, 5, 5, 2, 5, 5, 5, 2, 2, 2, 2, 2, 5, 5, 5, 0, 5, 5, 5, 1, 2, 2, 2, 2, 5, 5, 5, 5)\n(0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.25180384003913414, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.24336553748318454, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.06295096000978354, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.06295096000978354, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.12620765561942032, 0.12620765561942032, 0.12620765561942032, 0.12620765561942032, 0.06295096000978354, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.06295096000978354, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.060841384370796135, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.12620765561942032, 0.12620765561942032, 0.12620765561942032, 0.12620765561942032, 0.060841384370796135, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.12620765561942032, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.12620765561942032, 0.5048306224776813, 0.5048306224776813, 0.5048306224776813, 0.12620765561942032, 0.12620765561942032, 0.12620765561942032, 0.12620765561942032, 0.12620765561942032, 0.12620765561942032, 0.12620765561942032, 0.12620765561942032, 0.06295096000978354, 0.12620765561942032, 0.12620765561942032, 0.12620765561942032, 0.060841384370796135, 0.12620765561942032, 0.12620765561942032, 0.12620765561942032, 0.12620765561942032, 0.12620765561942032, 0.12620765561942032, 0.12620765561942032, 0.05609229138640903)\n1\n"
     ]
    }
   ],
   "source": [
    "# 3. ValueIteration\n",
    "mdpWarehouseValueIter = mdptoolbox.mdp.ValueIteration(T, R, 0.1)\n",
    "# Run the MDP\n",
    "mdpWarehouseValueIter.run()\n",
    "\n",
    "print('Value Iteration:')\n",
    "print(mdpWarehouseValueIter.policy)\n",
    "print(mdpWarehouseValueIter.V)\n",
    "print(mdpWarehouseValueIter.iter)"
   ]
  },
  {
   "source": [
    "Now we evaluate the models and compare the needed steps for each model and also compare them to a greedy approach."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('STORE', 'WHITE'), ('STORE', 'BLUE'), ('STORE', 'RED'), ('RESTORE', 'WHITE'), ('RESTORE', 'BLUE'), ('RESTORE', 'RED')]\n14401\n"
     ]
    }
   ],
   "source": [
    "stepsTaken = [1, 2, 2, 3]\n",
    "\n",
    "# get all test actions form the test file in a list\n",
    "def getTestActions(warehouseorder):\n",
    "    actionList = []\n",
    "    for line in warehouseorder:\n",
    "        split = line.split('\\t')\n",
    "        curOperation = split[0].upper()\n",
    "        curItem = split[1].strip('\\n').upper()\n",
    "        curAction = (curOperation, curItem)\n",
    "        actionList.append(curAction)\n",
    "    return actionList\n",
    "\n",
    "# evaluate a greedy approach, always store/restore at the nearest possible field\n",
    "def greedyStorage(actionList):\n",
    "    curState = ['EMPTY', 'EMPTY', 'EMPTY', 'EMPTY']\n",
    "    steps = 0\n",
    "\n",
    "    for (operation, item) in actionList:\n",
    "        # case operation not possible\n",
    "        if ((operation == 'STORE') and ('EMPTY' not in curState)) \\\n",
    "            or ((operation == 'RESTORE') and (item not in curState)):\n",
    "            print(\"does this happen?\") # => with our test data this is never the case\n",
    "        else:\n",
    "            if operation == 'STORE':\n",
    "                # store in the nearest field\n",
    "                for i in range(0, len(curState)):\n",
    "                    if curState[i] == 'EMPTY':\n",
    "                        curState[i] = item\n",
    "                        steps += stepsTaken[i]\n",
    "                        break\n",
    "            # restore operation\n",
    "            else:\n",
    "                for i in range(0, len(curState)):\n",
    "                    if curState[i] == item:\n",
    "                        curState[i] = 'EMPTY'\n",
    "                        steps += stepsTaken[i]\n",
    "                        break\n",
    "    return steps    \n",
    "\n",
    "print(actions)\n",
    "warehouseorder = open('Exercise4_warehousetraining2x2.txt')\n",
    "actionList = getTestActions(warehouseorder)\n",
    "greedySteps = greedyStorage(actionList)\n",
    "print(greedySteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Greedy approach steps:14401\n",
      "Policy iteration steps:15463\n",
      "Value iteration steps:15463\n",
      "Q learning steps:16353\n"
     ]
    }
   ],
   "source": [
    "# evaluate how much steps our mdp models will take:\n",
    "def evaluateMDPModel(mdpModel, actions, actionList):\n",
    "    steps = 0\n",
    "    policy = mdpModel.policy\n",
    "    V = mdpModel.V\n",
    "\n",
    "    curState = ['EMPTY', 'EMPTY', 'EMPTY', 'EMPTY']\n",
    "    for (operation, item) in actionList:\n",
    "        # case operation not possible\n",
    "        if ((operation == 'STORE') and ('EMPTY' not in curState)) \\\n",
    "            or ((operation == 'RESTORE') and (item not in curState)):\n",
    "            print(\"does this happen?\") # => with our test data this is never the case\n",
    "        else:\n",
    "            # 1. get next state based on value of the field?\n",
    "            curActionIndex = actions.index((operation, item))\n",
    "            indexCurState = states.index(curState)\n",
    "            curTransitionLine = T[curActionIndex, indexCurState]\n",
    "            curPossibleSteps = [i for i, value in enumerate(curTransitionLine) if value != 0]\n",
    "            valuesOfTheField = []\n",
    "            for posInd in curPossibleSteps:\n",
    "                valuesOfTheField.append(V[posInd])\n",
    "            max_value = max(valuesOfTheField)\n",
    "            max_index = curPossibleSteps[valuesOfTheField.index(max_value)]\n",
    "            # get next state\n",
    "            nextState = states[max_index].copy()\n",
    "            # evaluate steps, therefore get index of change:\n",
    "            for index, (first, second) in enumerate(zip(curState, nextState)):\n",
    "                if first != second:\n",
    "                    steps += stepsTaken[index]\n",
    "            # set cur state to next state:\n",
    "            curState = nextState           \n",
    "    return steps    \n",
    "\n",
    "stepsTaken = [1, 2, 2, 3]\n",
    "\n",
    "print(\"Greedy approach steps:\" + str(greedySteps))\n",
    "\n",
    "policyIterationSteps = evaluateMDPModel(mdpWarehousePolicy, actions, actionList)\n",
    "print(\"Policy iteration steps:\" + str(policyIterationSteps))\n",
    "\n",
    "valueIterationSteps = evaluateMDPModel(mdpWarehouseValueIter, actions, actionList)\n",
    "print(\"Value iteration steps:\" + str(valueIterationSteps))\n",
    "\n",
    "QlearningSteps = evaluateMDPModel(mdpWarehouseQ, actions, actionList)\n",
    "print(\"Q learning steps:\" + str(QlearningSteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}