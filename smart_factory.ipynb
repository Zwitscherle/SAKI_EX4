{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('saki4': venv)"
  },
  "interpreter": {
   "hash": "ad4ad34c08212da66dc6d04eda780af769d3ee710939179ad84caa50910b2cc9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This is the Smart Factory Exercise."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox, mdptoolbox.example\n",
    "import numpy as np\n",
    "import itertools\n",
    "from enum import Enum\n",
    "import math"
   ]
  },
  {
   "source": [
    "Define all items, states and actions of our model. <br>\n",
    "* We have three different items (WHITE, BLUE, RED)\n",
    "* We have four different possible states of each warehouse field (EMPTY, WHITE, BLUE, RED)\n",
    "* We have six possible actions for our agent (STORE and RESTORE in combination with each item color)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['WHITE', 'BLUE', 'RED']\n['WHITE', 'BLUE', 'RED', 'EMPTY']\n[('STORE', 'WHITE'), ('STORE', 'BLUE'), ('STORE', 'RED'), ('RESTORE', 'WHITE'), ('RESTORE', 'BLUE'), ('RESTORE', 'RED')]\n"
     ]
    }
   ],
   "source": [
    "items = ['WHITE', 'BLUE', 'RED']\n",
    "fieldStatus = ['WHITE', 'BLUE', 'RED', 'EMPTY']\n",
    "operations = ['STORE', 'RESTORE']\n",
    "actions = []\n",
    "for operation in operations:\n",
    "    for item in items:\n",
    "        actions.append((operation, item))\n",
    "\n",
    "# warehouse size n x n (in our case n = 2)\n",
    "n = 2\n",
    "print(items)\n",
    "print(fieldStatus)\n",
    "print(actions)"
   ]
  },
  {
   "source": [
    "Create all fields of the warehouse of size lenght x heigth (in our case 2 x 2)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "def createWarehouseFields(length, height):\n",
    "    warehouseFields = []\n",
    "    for i in range(0, length):\n",
    "        for j in range(0, height):\n",
    "            warehouseFields.append((i,j))\n",
    "    return warehouseFields\n",
    "\n",
    "warehouseFields = createWarehouseFields(n, n)\n",
    "print(warehouseFields)"
   ]
  },
  {
   "source": [
    "Next create all possible states of our warehouse. \n",
    "In our case we have 4 fields with 4 different states each (EMPTY, WHITE, BLUE, RED), which results in 4^4 states."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "def getStates(warehouseFields, fieldStatus):  \n",
    "    return itertools.product(fieldStatus, repeat=len(warehouseFields))\n",
    "\n",
    "iterStates = getStates(warehouseFields, fieldStatus)\n",
    "states = []\n",
    "for state in iterStates:\n",
    "  states.append(list(state))\n",
    "\n",
    "print(len(states))"
   ]
  },
  {
   "source": [
    "Create a reward which fits our problem. <br>\n",
    "The reward is higher if the distance our agent has to cover is lower."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.25180384 0.24336554 0.50483062]\n[(0, 0), (0, 1), (1, 0), (1, 1)]\n[1.0, 0.25, 0.25, 0.1111111111111111]\n"
     ]
    }
   ],
   "source": [
    "# read statistics from file to calculate better rewards\n",
    "# i = 0: White, i = 1: Blue, i = 2: Red\n",
    "countItems = np.zeros((3))\n",
    "itemsTotal = 0\n",
    "\n",
    "warehouseorder = open('Exercise4_warehousetraining2x2.txt')\n",
    "for line in warehouseorder:\n",
    "    curAction = line.split('\\t')\n",
    "    # curOperation = curAction[0].upper()\n",
    "    curItem = curAction[1].strip('\\n').upper()\n",
    "    if curItem == 'WHITE':\n",
    "        countItems[0] += 1\n",
    "    if curItem == 'BLUE':\n",
    "        countItems[1] += 1\n",
    "    if curItem == 'RED':\n",
    "        countItems[2] += 1\n",
    "    itemsTotal += 1\n",
    "\n",
    "# i = 0: White, i = 1: Blue, i = 2: Red\n",
    "probsItems = countItems / itemsTotal\n",
    "print(probsItems)\n",
    "\n",
    "# reward function based on distance and item probability:\n",
    "def getRewardBasedOnDistanceAndProbability(fieldIndex, item):\n",
    "    x = warehouseFields[fieldIndex]\n",
    "    distance = x[0] + x[1] + 1\n",
    "    prob = 0\n",
    "    if item == 'WHITE':\n",
    "        prob = probsItems[0]\n",
    "    if item == 'BLUE':\n",
    "        prob = probsItems[1]\n",
    "    if item == 'RED':\n",
    "        prob = probsItems[2]\n",
    "    reward = 1/distance * 1/distance * prob\n",
    "    return reward\n",
    "\n",
    "\n",
    "# simple reward function:\n",
    "# get the distance in field from 0,0 (0,0 has already distance 1)\n",
    "# reward is the reciprocal**2 (to punish far away fields even more)\n",
    "def getRewardBasedOnDistance(x):\n",
    "    distance = x[0] + x[1] + 1\n",
    "    reward = 1/distance * 1/distance\n",
    "    return reward\n",
    "\n",
    "rewardVector = [getRewardBasedOnDistance(x) for x in warehouseFields]\n",
    "print(warehouseFields)\n",
    "print(rewardVector)"
   ]
  },
  {
   "source": [
    "Bring it all together now and create the transition and the reward matrix. <br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(6, 256, 256)\n1536.0\n"
     ]
    }
   ],
   "source": [
    "# create Transition and reward matrix\n",
    "def createTransitionAndRewardMatrix(actions, states):\n",
    "    T = np.zeros((len(actions), len(states), len(states)))\n",
    "    R = np.zeros((len(states), len(actions)))\n",
    "\n",
    "    for i in range(len(actions)):\n",
    "        # current operation and item\n",
    "        operation, item = actions[i]\n",
    "\n",
    "        for j in range(len(states)):\n",
    "            curState = states[j]\n",
    "\n",
    "            # if an operation is not valid i.e. store when warehouse is full \n",
    "            # or restore when warehouse is empty, stay in the current state \n",
    "            if ((operation == 'STORE') and ('EMPTY' not in curState)) \\\n",
    "                or ((operation == 'RESTORE') and (item not in curState)):\n",
    "                T[i, j, j] = 1                \n",
    "            else:\n",
    "                possibleFields = []\n",
    "                for k in range(len(curState)):\n",
    "                    # find empty fields where the agent can store the item\n",
    "                    if operation == 'STORE':\n",
    "                        if curState[k] == 'EMPTY':\n",
    "                            possibleFields.append(k)\n",
    "                    # find fields with the requested item\n",
    "                    else:\n",
    "                        if curState[k] == item:\n",
    "                            possibleFields.append(k)\n",
    "                # possible fields can not be empty because of the check above\n",
    "                transitionProbability = 1 / len(possibleFields)\n",
    "                # set probabiltiy in transition matrix for the possible nextStates\n",
    "                for field in possibleFields:\n",
    "                    nextState = curState.copy()\n",
    "                    if operation == 'STORE':\n",
    "                        nextState[field] = item\n",
    "                    else:\n",
    "                        nextState[field] = 'EMPTY'\n",
    "                    nextIndex = states.index(nextState)\n",
    "                    T[i, j, nextIndex] = transitionProbability\n",
    "                    # set reward in R\n",
    "                    # R[nextIndex][i] = rewardVector[field] # old simple reward\n",
    "                    R[nextIndex][i] = getRewardBasedOnDistanceAndProbability(field, item)         \n",
    "    return T, R\n",
    "\n",
    "T, R = createTransitionAndRewardMatrix(actions, states)\n",
    "print(np.shape(T))\n",
    "test = np.sum(T)\n",
    "print(test)"
   ]
  },
  {
   "source": [
    "Finally create the mdp models and evaluate the different classes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PolicyIteration:\n",
      "(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 2, 0, 0, 2, 0, 5, 5, 5, 0, 5, 5, 5, 1, 5, 5, 5, 2, 1, 1, 2, 1, 5, 5, 5, 2, 5, 5, 5, 2, 5, 5, 5, 2, 2, 2, 2, 2, 0, 0, 2, 0, 1, 1, 2, 1, 2, 2, 2, 2, 5, 5, 2, 2)\n",
      "(0.35971977148447737, 0.35971977148447737, 0.35971977148447737, 0.3597197714844773, 0.35971977148447737, 0.35971977148447737, 0.35971977148447737, 0.3597197714844773, 0.35971977148447737, 0.35971977148447737, 0.35971977148447737, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.35971977148447737, 0.35971977148447737, 0.35971977148447737, 0.3597197714844773, 0.35971977148447737, 0.35971977148447737, 0.35971977148447737, 0.3597197714844773, 0.35971977148447737, 0.35971977148447737, 0.35971977148447737, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.35971977148447737, 0.35971977148447737, 0.35971977148447737, 0.3597197714844773, 0.35971977148447737, 0.35971977148447737, 0.35971977148447737, 0.3597197714844773, 0.35971977148447737, 0.35971977148447737, 0.35971977148447737, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.3597197714844773, 0.35971977148447737, 0.3476650535474065, 0.3476650535474065, 0.3476650535474065, 0.34766505354740646, 0.3476650535474065, 0.3476650535474065, 0.3476650535474065, 0.34766505354740646, 0.3476650535474065, 0.3476650535474065, 0.3476650535474065, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.3476650535474065, 0.3476650535474065, 0.3476650535474065, 0.34766505354740646, 0.3476650535474065, 0.3476650535474065, 0.3476650535474065, 0.34766505354740646, 0.3476650535474065, 0.3476650535474065, 0.3476650535474065, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.3476650535474065, 0.3476650535474065, 0.3476650535474065, 0.34766505354740646, 0.3476650535474065, 0.3476650535474065, 0.3476650535474065, 0.34766505354740646, 0.3476650535474065, 0.3476650535474065, 0.3476650535474065, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.34766505354740646, 0.7211866035395448, 0.7211866035395448, 0.7211866035395448, 0.7211866035395447, 0.7211866035395448, 0.7211866035395448, 0.7211866035395448, 0.7211866035395447, 0.7211866035395448, 0.7211866035395448, 0.7211866035395448, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395448, 0.7211866035395448, 0.7211866035395448, 0.7211866035395447, 0.7211866035395448, 0.7211866035395448, 0.7211866035395448, 0.7211866035395447, 0.7211866035395448, 0.7211866035395448, 0.7211866035395448, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395448, 0.7211866035395448, 0.7211866035395448, 0.7211866035395447, 0.7211866035395448, 0.7211866035395448, 0.7211866035395448, 0.7211866035395447, 0.7211866035395448, 0.7211866035395448, 0.7211866035395448, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395447, 0.7211866035395446, 0.7211866035395448, 0.7211866035395448, 0.5723566973566974, 0.2250869162633868, 0.7211866035395448, 0.7211866035395448, 0.5723566973566974, 0.2250869162633868, 0.5723566973566974, 0.5723566973566974, 0.5912345672123929, 0.3230708312322109, 0.2250869162633868, 0.2250869162633868, 0.25295546699919963, 0.14394032041090862, 0.7211866035395448, 0.7211866035395448, 0.5723566973566974, 0.2250869162633868, 0.7211866035395448, 0.7211866035395448, 0.5711813623578329, 0.2211691329338388, 0.5711813623578329, 0.5711813623578329, 0.5912345672123929, 0.3230708312322109, 0.2211691329338388, 0.2211691329338388, 0.25295546699919963, 0.13984171631230452, 0.5723566973566974, 0.5723566973566974, 0.5912345672123929, 0.3230708312322109, 0.5711813623578329, 0.5711813623578329, 0.5912345672123929, 0.3230708312322109, 0.6022474516468972, 0.6022474516468972, 0.6022474516468974, 0.3247227638973866, 0.32472276389738663, 0.32472276389738663, 0.32472276389738663, 0.2632708687528521, 0.2250869162633868, 0.2250869162633868, 0.25295546699919963, 0.14394032041090862, 0.2211691329338388, 0.2211691329338388, 0.25295546699919963, 0.13984171631230452, 0.3247227638973866, 0.3247227638973866, 0.32472276389738663, 0.2632708687528521, 0.1802966508848862, 0.1802966508848862, 0.19315550451984081, 0.10806628841738172)\n",
      "2\n",
      "Q learning:\n",
      "RelativeValueIteration:\n",
      "Value Iteration:\n"
     ]
    }
   ],
   "source": [
    "# 1. Policy Iteration\n",
    "mdpWarehousePolicy = mdptoolbox.mdp.PolicyIteration(T, R, 0.3, max_iter=100)\n",
    "# Run the MDP\n",
    "mdpWarehousePolicy.run()\n",
    "\n",
    "print('PolicyIteration:')\n",
    "print(mdpWarehousePolicy.policy)\n",
    "print(mdpWarehousePolicy.V)\n",
    "print(mdpWarehousePolicy.iter)\n",
    "\n",
    "# 2. QLearning\n",
    "mdpWarehouseQ = mdptoolbox.mdp.QLearning(T, R, 0.3)\n",
    "# Run the MDP\n",
    "mdpWarehouseQ.run()\n",
    "\n",
    "print('Q learning:')\n",
    "#print(mdpWarehouseQ.policy)\n",
    "#print(mdpWarehouseQ.V)\n",
    "\n",
    "# 3. RelativeValueIteration:\n",
    "mdpWarehouseRelIter = mdptoolbox.mdp.RelativeValueIteration(T, R, 0.3)\n",
    "# Run the MDP\n",
    "mdpWarehouseRelIter.run()\n",
    "\n",
    "print('RelativeValueIteration:')\n",
    "#print(mdpWarehouseRelIter.policy)\n",
    "#print(mdpWarehouseRelIter.V)\n",
    "#print(mdpWarehouseRelIter.iter)\n",
    "\n",
    "# 4. ValueIteration\n",
    "mdpWarehouseValueIter = mdptoolbox.mdp.ValueIteration(T, R, 0.3)\n",
    "# Run the MDP\n",
    "mdpWarehouseValueIter.run()\n",
    "\n",
    "print('Value Iteration:')\n",
    "#print(mdpWarehouseValueIter.policy)\n",
    "#print(mdpWarehouseValueIter.V)\n",
    "#print(mdpWarehouseValueIter.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}