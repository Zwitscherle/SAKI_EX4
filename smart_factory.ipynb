{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('saki4': venv)"
  },
  "interpreter": {
   "hash": "ad4ad34c08212da66dc6d04eda780af769d3ee710939179ad84caa50910b2cc9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This is the Smart Factory Exercise."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox, mdptoolbox.example\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "source": [
    "Define all items, states and actions of our model. <br>\n",
    "* We have three different items (WHITE, BLUE, RED)\n",
    "* We have four different possible states of each warehouse field (EMPTY, WHITE, BLUE, RED)\n",
    "* We have two operations (STORE, RESTORE) which we can combine with each item\n",
    "* Our robot can take n x n actions (It can chose one of the warehousefields)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n[('STORE', 'WHITE'), ('STORE', 'BLUE'), ('STORE', 'RED'), ('RESTORE', 'WHITE'), ('RESTORE', 'BLUE'), ('RESTORE', 'RED')]\n"
     ]
    }
   ],
   "source": [
    "def createWarehouseFields(length, height):\n",
    "    warehouseFields = []\n",
    "    for i in range(0, length):\n",
    "        for j in range(0, height):\n",
    "            warehouseFields.append((i,j))\n",
    "    return warehouseFields\n",
    "\n",
    "# warehouse size n x n (in our case n = 2)\n",
    "n = 2\n",
    "warehouseFields = createWarehouseFields(n, n)\n",
    "actions = warehouseFields.copy()\n",
    "\n",
    "items = ['WHITE', 'BLUE', 'RED']\n",
    "operations = ['STORE', 'RESTORE']\n",
    "warehouseState = ['WHITE', 'BLUE', 'RED', 'EMPTY']\n",
    "operationsWithItems = []\n",
    "for operation in operations:\n",
    "    for item in items:\n",
    "        operationsWithItems.append((operation, item))\n",
    "\n",
    "print(actions)\n",
    "print(operationsWithItems)"
   ]
  },
  {
   "source": [
    "Next create all possible states of our warehouse, which are our operations with items (2x3) * each of the warehouse sates (nxn) ** Warehousestates.\n",
    "This gives us 2*3*4^4= 1536 states-   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total amount of states:1536\n"
     ]
    }
   ],
   "source": [
    "def getStates(warehouseFields, fieldStatus, operationsWithItems):  \n",
    "    warehouseStates = itertools.product(fieldStatus, repeat=len(warehouseFields))\n",
    "    states = []\n",
    "    for state in warehouseStates:\n",
    "      states.append(list(state))\n",
    "    statesWithOperations = []\n",
    "    for operation in operationsWithItems:\n",
    "      for state in states:\n",
    "        newCompleteState = state.copy()\n",
    "        newCompleteState = [operation] + newCompleteState\n",
    "        statesWithOperations.append(newCompleteState)\n",
    "    return statesWithOperations\n",
    "\n",
    "iterStates = getStates(warehouseFields, warehouseState, operationsWithItems)\n",
    "# convert states to list\n",
    "states = []\n",
    "for state in iterStates:\n",
    "  states.append(list(state))\n",
    "\n",
    "print(\"Total amount of states:\" + str(len(states)))"
   ]
  },
  {
   "source": [
    "Read the information from the training file to get solid probabilites for each state transition. <br>\n",
    "Create an reward function that fits our problem."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.12596307 0.12168277 0.25241531 0.12584077 0.12168277 0.25241531]\n"
     ]
    }
   ],
   "source": [
    "# read statistics from file to get a solid transition function\n",
    "# i = 0: ('STORE', 'WHITE'), i = 1: ('STORE', 'BLUE'), i = 2: ('STORE', 'RED')\n",
    "# i = 3: ('RESTORE', 'WHITE'), i = 4: ('RESTORE', 'BLUE'), i = 5: ('RESTORE', 'RED')\n",
    "countItems = np.zeros((6))\n",
    "itemsTotal = 0\n",
    "\n",
    "warehouseorder = open('Exercise4_warehousetraining2x2.txt')\n",
    "for line in warehouseorder:\n",
    "    curAction = line.split('\\t')\n",
    "    curOperation = curAction[0].upper()\n",
    "    curItem = curAction[1].strip('\\n').upper()\n",
    "    curOperationWithItem = (curOperation, curItem)\n",
    "    if curOperationWithItem == ('STORE', 'WHITE'):\n",
    "        countItems[0] += 1\n",
    "    elif curOperationWithItem == ('STORE', 'BLUE'):\n",
    "        countItems[1] += 1\n",
    "    elif curOperationWithItem == ('STORE', 'RED'):\n",
    "        countItems[2] += 1\n",
    "    elif curOperationWithItem == ('RESTORE', 'WHITE'):\n",
    "        countItems[3] += 1\n",
    "    elif curOperationWithItem == ('RESTORE', 'BLUE'):\n",
    "        countItems[4] += 1\n",
    "    else:\n",
    "        countItems[5] += 1\n",
    "    itemsTotal += 1\n",
    "\n",
    "operationsWithItemsProbabilites = countItems / itemsTotal\n",
    "print(operationsWithItemsProbabilites)\n",
    "\n",
    "\n",
    "# get probabilites based on the operation and operation x item probabilites\n",
    "def getTransitionProbabiltiy(operation):\n",
    "    if operation == ('STORE', 'WHITE'):\n",
    "        return operationsWithItemsProbabilites[0]\n",
    "    elif operation == ('STORE', 'BLUE'):\n",
    "        return operationsWithItemsProbabilites[1]\n",
    "    elif operation == ('STORE', 'RED'):\n",
    "        return operationsWithItemsProbabilites[2]\n",
    "    elif operation == ('RESTORE', 'WHITE'):\n",
    "        return operationsWithItemsProbabilites[3]\n",
    "    elif operation == ('RESTORE', 'BLUE'):\n",
    "        return operationsWithItemsProbabilites[4]\n",
    "    else:\n",
    "        return operationsWithItemsProbabilites[5]\n",
    "\n",
    "# function that calculates a simple reward for each field => better reward if the distance is low\n",
    "def getSimpleReward(warehouseFields):\n",
    "    reward = []\n",
    "    for (x, y) in warehouseFields:\n",
    "        distance = (x+y+1)\n",
    "        curReward = 1/distance * 1/distance\n",
    "        reward.append(curReward)\n",
    "    return reward"
   ]
  },
  {
   "source": [
    "Bring it all together now and create the transition and the reward matrix. <br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4, 1536, 1536)\n1.0\n"
     ]
    }
   ],
   "source": [
    "# create Transition and reward matrix\n",
    "def createTransitionAndRewardMatrix(actions, states, numberOperations, rewardFunction):\n",
    "    numberActions = len(actions)\n",
    "    numberStates = len(states)\n",
    "    # the warehouse state repeats in an iterval for each operation\n",
    "    operationInterval = len(states) / numberOperations\n",
    "    T = np.zeros((numberActions, numberStates, numberStates))\n",
    "    R = np.zeros((numberActions, numberStates, numberStates))\n",
    "\n",
    "    for i in range(numberActions):\n",
    "        # current action which is the field we take\n",
    "        action = actions[i]\n",
    "        for j in range(len(states)):\n",
    "            curState = states[j]           \n",
    "            operation = curState[0]\n",
    "            curWarehouseState = curState[1:].copy()\n",
    "            # check if we are in an invalid scenario => no reward and do not change warehouse state\n",
    "            if (curWarehouseState[i] != 'EMPTY' and operation[0] == 'STORE') \\\n",
    "                or (curWarehouseState[i] != operation[1] and operation[0] == 'RESTORE'):\n",
    "                for k in range(numberOperations):\n",
    "                    nextIndex = int((j + k * operationInterval) % len(states))\n",
    "                    nextOperation = states[nextIndex][0]\n",
    "                    T[i, j, nextIndex] =  getTransitionProbabiltiy(nextOperation)\n",
    "            # valid operation set reward and change warehouse state\n",
    "            else:\n",
    "                nextWarehouseState = curWarehouseState\n",
    "                if operation == 'STORE':\n",
    "                    nextWarehouseState[i] = operation[1]\n",
    "                else:\n",
    "                    nextWarehouseState[i] = 'EMPTY'\n",
    "                nextState = [operation] + nextWarehouseState\n",
    "                nextIndexStart = states.index(nextState)\n",
    "                for k in range(numberOperations):\n",
    "                    nextIndex = int((nextIndexStart + operationInterval*k) % len(states))\n",
    "                    nextOperation = states[nextIndex][0]\n",
    "                    T[i, j, nextIndex] = getTransitionProbabiltiy(nextOperation)\n",
    "                    R[i, j, nextIndex] = rewardFunction[i]   \n",
    "    return T, R\n",
    "\n",
    "rewardFunction = getSimpleReward(warehouseFields)\n",
    "T, R = createTransitionAndRewardMatrix(actions, states, len(operationsWithItems), rewardFunction)\n",
    "print(np.shape(T))\n",
    "test = np.sum(T[0][0])\n",
    "print(test)"
   ]
  },
  {
   "source": [
    "Finally create the mdp models and evaluate the different classes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PolicyIteration:\n(0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 2, 2, 2, 2, 0, 0, 0, 3)\n(0.014603873152815833, 0.016166200060012882, 0.01779896409443149, 0.13180344388096357, 0.0181187495384474, 0.018138230885186263, 0.021309325509641642, 0.13523791840022625, 0.02179246861588927, 0.023350280552664904, 0.021874872158246024, 0.13882788131383167, 0.2783025481355866, 0.27978447317641475, 0.2813334810469968, 0.27830225679367804, 0.0181187495384474, 0.018138230885186263, 0.021309325509641642, 0.13523791840022625)\n1\n"
     ]
    }
   ],
   "source": [
    "# 1. Policy Iteration\n",
    "mdpWarehousePolicy = mdptoolbox.mdp.PolicyIteration(T, R, 0.1, max_iter=100)\n",
    "# Run the MDP\n",
    "mdpWarehousePolicy.run()\n",
    "\n",
    "# just show the first 20 entries of the matrices\n",
    "print('PolicyIteration:')\n",
    "print(mdpWarehousePolicy.policy[0:20])\n",
    "print(mdpWarehousePolicy.V[0:20])\n",
    "print(mdpWarehousePolicy.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Q learning:\n(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.7074934631172406e-06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# 2. QLearning\n",
    "mdpWarehouseQ = mdptoolbox.mdp.QLearning(T, R, 0.1)\n",
    "# Run the MDP\n",
    "mdpWarehouseQ.run()\n",
    "\n",
    "print('Q learning:')\n",
    "print(mdpWarehouseQ.policy[0:20]) # I think this chooses 0 way to often, might be an error in the model\n",
    "print(mdpWarehouseQ.V[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Value Iteration:\n(0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 2, 2, 2, 2, 0, 0, 0, 3)\n(0.012584077289959646, 0.013936108053755114, 0.015388691859280096, 0.12925142336907042, 0.01562614650849945, 0.01562614650849945, 0.018430761077819904, 0.13229349258761022, 0.018894460070930662, 0.02024649083472613, 0.018894460070930662, 0.13556180615004143, 0.2750856059679589, 0.27643763673175437, 0.27789022053727935, 0.2750856059679589, 0.01562614650849945, 0.01562614650849945, 0.018430761077819904, 0.13229349258761022)\n2\n"
     ]
    }
   ],
   "source": [
    "# 3. ValueIteration\n",
    "mdpWarehouseValueIter = mdptoolbox.mdp.ValueIteration(T, R, 0.1, max_iter=100)\n",
    "# Run the MDP\n",
    "mdpWarehouseValueIter.run()\n",
    "\n",
    "print('Value Iteration:')\n",
    "print(mdpWarehouseValueIter.policy[0:20])\n",
    "print(mdpWarehouseValueIter.V[0:20])\n",
    "print(mdpWarehouseValueIter.iter)"
   ]
  },
  {
   "source": [
    "Now we evaluate the models and compare the needed steps for each model and also compare them to a greedy approach."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14401\n"
     ]
    }
   ],
   "source": [
    "# get all test actions form the test file in a list\n",
    "def getTestActions(warehouseorder):\n",
    "    actionList = []\n",
    "    for line in warehouseorder:\n",
    "        split = line.split('\\t')\n",
    "        curOperation = split[0].upper()\n",
    "        curItem = split[1].strip('\\n').upper()\n",
    "        curAction = (curOperation, curItem)\n",
    "        actionList.append(curAction)\n",
    "    return actionList\n",
    "\n",
    "# evaluate a greedy approach, always store/restore at the nearest possible field\n",
    "def greedyStorage(actionList, stepsTaken):\n",
    "    curState = ['EMPTY', 'EMPTY', 'EMPTY', 'EMPTY']\n",
    "    steps = 0\n",
    "\n",
    "    for (operation, item) in actionList:\n",
    "        # case operation not possible\n",
    "        if ((operation == 'STORE') and ('EMPTY' not in curState)) \\\n",
    "            or ((operation == 'RESTORE') and (item not in curState)):\n",
    "            print(\"does this happen?\") # => with our test data this is never the case\n",
    "        else:\n",
    "            if operation == 'STORE':\n",
    "                # store in the nearest field\n",
    "                for i in range(0, len(curState)):\n",
    "                    if curState[i] == 'EMPTY':\n",
    "                        curState[i] = item\n",
    "                        steps += stepsTaken[i]\n",
    "                        break\n",
    "            # restore operation\n",
    "            else:\n",
    "                for i in range(0, len(curState)):\n",
    "                    if curState[i] == item:\n",
    "                        curState[i] = 'EMPTY'\n",
    "                        steps += stepsTaken[i]\n",
    "                        break\n",
    "    return steps    \n",
    "\n",
    "stepsTaken = []\n",
    "for (x, y) in warehouseFields:\n",
    "    stepsTaken.append(x+y+1)\n",
    "\n",
    "warehouseorder = open('Exercise4_warehousetraining2x2.txt')\n",
    "actionList = getTestActions(warehouseorder)\n",
    "greedySteps = greedyStorage(actionList, stepsTaken)\n",
    "print(greedySteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Greedy approach steps:14401\n",
      "Policy iteration steps:14401\n",
      "Value iteration steps:14401\n",
      "Q learning steps:9137\n"
     ]
    }
   ],
   "source": [
    "# evaluate how much steps our mdp models will take:\n",
    "def evaluateMDPModel(mdpModel, actions, actionList):\n",
    "    steps = 0\n",
    "    policy = mdpModel.policy\n",
    "    actionSteps = [1,2,2,3]\n",
    "    curState = ['EMPTY', 'EMPTY', 'EMPTY', 'EMPTY']\n",
    "    # iterate through actions, transition via policy\n",
    "    for action in actionList:\n",
    "        # get index of the current state\n",
    "        curStateIndex = states.index([action] + curState)\n",
    "        # select the action based on the policy\n",
    "        actionField = policy[curStateIndex]\n",
    "        steps += actionSteps[actionField]\n",
    "        # change warehouse state for the next action\n",
    "        (operation, item) = action\n",
    "        if operation == 'STORE':\n",
    "            # only happens with qlearning, policy and value iteration only take valid actions\n",
    "            #if curState[actionField] != 'EMPTY':\n",
    "            #     print(\"not allowed action store\")\n",
    "            curState[actionField] = item\n",
    "        else:\n",
    "            #if curState[actionField] != item:\n",
    "            #    print(\"not allowed action restore\")\n",
    "            curState[actionField] = 'EMPTY'\n",
    "    return steps    \n",
    "\n",
    "print(\"Greedy approach steps:\" + str(greedySteps))\n",
    "\n",
    "policyIterationSteps = evaluateMDPModel(mdpWarehousePolicy, actions, actionList)\n",
    "print(\"Policy iteration steps:\" + str(policyIterationSteps))\n",
    "\n",
    "valueIterationSteps = evaluateMDPModel(mdpWarehouseValueIter, actions, actionList)\n",
    "print(\"Value iteration steps:\" + str(valueIterationSteps))\n",
    "\n",
    "# Qlearning cheats, it takes options which are not valid as it is configured right now\n",
    "QlearningSteps = evaluateMDPModel(mdpWarehouseQ, actions, actionList)\n",
    "print(\"Q learning steps:\" + str(QlearningSteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}