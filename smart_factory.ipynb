{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('saki4': venv)"
  },
  "interpreter": {
   "hash": "ad4ad34c08212da66dc6d04eda780af769d3ee710939179ad84caa50910b2cc9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This is the smart factory exercise\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox, mdptoolbox.example\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "Hints: define states (1572864) + distance influences reward\n",
    "goal: minimize distance \n",
    "items: white blue red\n",
    "\n",
    "create a warehouse of size 3x3, with a separate start \n",
    "-> first move of the agent is always to 1x1\n",
    "\n",
    "MDP 4-tuple(State, Actions, Probability Transition matrix P_a(s,s'), Reward matrix R_a(s,s'))\n",
    "\n",
    "possible States: <br>\n",
    "start/end point: <br>\n",
    "agent start, agent end, empty, white, blue, red => 6 states <br>\n",
    "storage fields: <br> \n",
    "empty, white, blue, red with <br>\n",
    "=> 4 states, but what is with the agent? <br>\n",
    "would make the 1572864 states"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.  1.  0.  0.  0. ]\n [0.  0.  0.5 0.5 0. ]\n [0.  0.  0.  0.  1. ]\n [0.  0.  0.  0.  1. ]\n [0.  0.  0.  0.  0. ]]\n[[1.         0.5        0.33333333 0.5        0.33333333]\n [0.         1.         0.         0.         0.        ]\n [0.         1.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Transition matrix for 2x2 warehouse + start position:\n",
    "# only one direction\n",
    "T = np.zeros((5,5))\n",
    "T[0][1] = 1 \n",
    "T[1] = [0, 0, 1/2, 1/2, 0]\n",
    "T[2] = [0, 0, 0, 0, 1]\n",
    "T[3] = [0, 0, 0, 0, 1]\n",
    "T[4] = [0, 0, 0, 0, 0]\n",
    "print(T)\n",
    "\n",
    "# Reward Matrix:\n",
    "# make a reward which gets worse for each step => goal minimize distance\n",
    "# measure steps from start to storage place for each state:\n",
    "# 1: 1, 2,3: 2, 4: 3 => for reward take reciprocal\n",
    "# Action A, B in this case\n",
    "R = np.zeros((2,5))\n",
    "R[0] = [1, 1/2, 1/3, 1/3, 0] # action A\n",
    "R[1] = [0, 1/2, 0, 0, 0] # action B\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.         1.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.33333333 0.         0.33333333 0.         0.33333333 0.\n  0.         0.         0.         0.        ]\n [0.         0.33333333 0.         0.33333333 0.         0.33333333\n  0.         0.         0.         0.        ]\n [0.         0.         0.5        0.         0.         0.\n  0.5        0.         0.         0.        ]\n [0.         0.33333333 0.         0.         0.         0.33333333\n  0.         0.33333333 0.         0.        ]\n [0.         0.         0.25       0.         0.25       0.\n  0.25       0.         0.25       0.        ]\n [0.         0.         0.         0.33333333 0.         0.33333333\n  0.         0.         0.         0.33333333]\n [0.         0.         0.         0.         0.5        0.\n  0.         0.         0.5        0.        ]\n [0.         0.         0.         0.         0.         0.33333333\n  0.         0.33333333 0.         0.33333333]\n [0.         0.         0.         0.         0.         0.\n  0.5        0.         0.5        0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Transition matrix for 3x3 warehouse + start position(index 0) (10 x 10?):\n",
    "# start state = 0, (1,1) = 1, (1,2) = 2, (1,3) = 3, (2,1) = 4, (2,2) = 5, (2,3) = 6\n",
    "# (3,1) = 7, (3,2) = 8, (3,3) = 9\n",
    "#T = np.zeros((10,10))\n",
    "#T[0][1] = 1 \n",
    "#T[1] = [1/3, 0, 1/3, 0, 1/3, 0, 0, 0, 0, 0]\n",
    "#T[2] = [0, 1/3, 0, 1/3, 0, 1/3, 0, 0, 0, 0]\n",
    "#T[3] = [0, 0, 1/2, 0, 0, 0, 1/2, 0, 0, 0]\n",
    "#T[4] = [0, 1/3, 0, 0, 0, 1/3, 0, 1/3, 0, 0]\n",
    "#T[5] = [0, 0, 1/4, 0, 1/4, 0, 1/4, 0, 1/4, 0]\n",
    "#T[6] = [0, 0, 0, 1/3, 0, 1/3, 0, 0, 0, 1/3]\n",
    "#T[7] = [0, 0, 0, 0, 1/2, 0, 0, 0, 1/2, 0]\n",
    "#T[8] = [0, 0, 0, 0, 0, 1/3, 0, 1/3, 0, 1/3]\n",
    "#T[9] = [0, 0, 0, 0, 0, 0, 1/2, 0, 1/2, 0]\n",
    "#print(T)\n",
    "\n",
    "# Reward Matrix:\n",
    "# make a reward which gets worse for each step => goal minimize distance\n",
    "# measure steps from start to storage place for each state:\n",
    "# 1: 1, 2,4: 2, 3,5,7: 3, 6,8: 4, 9:5 => for reward take reciprocal\n",
    "# TODO\n",
    "#R = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mdp\n",
    "# just test\n",
    "#mdp = mdptoolbox.mdp.MDP()"
   ]
  }
 ]
}